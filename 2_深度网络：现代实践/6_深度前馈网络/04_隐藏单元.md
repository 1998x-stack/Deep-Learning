# 04_隐藏单元
"""
Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 04_隐藏单元
"""
## 04_隐藏单元
### 任务分解：
1. **背景介绍**
2. **隐藏单元的定义**
3. **常见的隐藏单元类型**
4. **选择合适的隐藏单元**
5. **隐藏单元在神经网络中的实现**
6. **训练和评估模型**
7. **可视化隐藏单元的激活函数**
### 1. 背景介绍
**步骤：**
- 解释隐藏单元在神经网络中的作用。
- 强调隐藏单元对模型表达能力和性能的重要性。
**解释：**
隐藏单元是神经网络的中间层，用于提取和学习输入数据的特征。隐藏单元通过非线性激活函数处理输入，从而增加网络的表达能力，能够学习复杂的非线性关系。
### 2. 隐藏单元的定义
**步骤：**
- 提供隐藏单元的数学定义。
- 说明隐藏单元如何在神经网络中进行信息处理。
**解释：**
隐藏单元接收前一层的输入，经过线性变换和非线性激活函数后，输出给下一层。数学上，隐藏单元的输出可以表示为：
$$ h = \sigma(Wx + b) $$
其中，$ \sigma $ 是激活函数，$ W $ 是权重矩阵，$ x $ 是输入，$ b $ 是偏置。
### 3. 常见的隐藏单元类型
**步骤：**
- 介绍常见的隐藏单元类型及其激活函数：
  - ReLU（Rectified Linear Unit）
  - Sigmoid
  - Tanh
  - Leaky ReLU
  - ELU（Exponential Linear Unit）
**解释：**
- **ReLU（Rectified Linear Unit）**：ReLU 是目前最常用的激活函数，定义为：
  $$ f(x) = \max(0, x) $$
  ReLU 的优点是计算简单，并且可以缓解梯度消失问题。
- **Sigmoid**：Sigmoid 函数将输入压缩到 [0, 1] 之间，定义为：
  $$ f(x) = \frac{1}{1 + e^{-x}} $$
  Sigmoid 在深层网络中可能会导致梯度消失问题。
- **Tanh**：Tanh 函数将输入压缩到 [-1, 1] 之间，定义为：
  $$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
  Tanh 与 Sigmoid 类似，但输出范围更大，梯度消失问题稍微缓解。
- **Leaky ReLU**：Leaky ReLU 是 ReLU 的改进版本，当输入为负时，允许一定的负斜率，定义为：
  $$ f(x) = \max(0.01x, x) $$
- **ELU（Exponential Linear Unit）**：ELU 在负输入时具有指数性质，定义为：
  $$ f(x) = \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha (e^x - 1) & \text{if } x \leq 0 
   \end{cases} $$
### 4. 选择合适的隐藏单元
**步骤：**
- 讨论如何根据具体任务选择合适的隐藏单元。
- 说明不同隐藏单元对模型性能的影响。
**解释：**
选择合适的隐藏单元取决于具体的任务和网络深度。例如，ReLU 适用于大多数任务，尤其是深层网络，因为它计算简单且能有效缓解梯度消失问题。而在一些特殊任务中，可能需要使用 Sigmoid、Tanh 或其他激活函数。
### 5. 隐藏单元在神经网络中的实现
**步骤：**
- 使用 PyTorch 实现常见的隐藏单元。
- 演示如何在神经网络中使用这些隐藏单元。
**代码：**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self, hidden_unit='relu'):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        if hidden_unit == 'relu':
            self.hidden = nn.ReLU()
        elif hidden_unit == 'sigmoid':
            self.hidden = nn.Sigmoid()
        elif hidden_unit == 'tanh':
            self.hidden = nn.Tanh()
        elif hidden_unit == 'leaky_relu':
            self.hidden = nn.LeakyReLU()
        elif hidden_unit == 'elu':
            self.hidden = nn.ELU()
        self.fc2 = nn.Linear(2, 1)
        self.output = nn.Sigmoid()
    def forward(self, x):
        x = self.hidden(self.fc1(x))
        x = self.output(self.fc2(x))
        return x
# 初始化模型
model = SimpleNN(hidden_unit='relu')
# 定义损失函数和优化器
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
```
### 6. 训练和评估模型
**步骤：**
- 训练模型并记录损失。
- 评估模型在训练数据上的准确性。
**代码：**
```python
# 训练和评估模型的代码已包含在上一步中
```
### 7. 可视化隐藏单元的激活函数
**步骤：**
- 可视化不同隐藏单元的激活函数输出。
- 展示不同隐藏单元在训练过程中的表现差异。
**代码：**
```python
import matplotlib.pyplot as plt
# 可视化训练损失
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss with ReLU Hidden Unit')
plt.show()
```

# 05_整流线性单元及其扩展
"""
Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 05_整流线性单元及其扩展
"""
## 05_整流线性单元及其扩展
### 任务分解：
1. **背景介绍**
2. **整流线性单元（ReLU）的定义**
3. **ReLU 的优缺点**
4. **ReLU 的扩展**
5. **实现 ReLU 及其扩展**
6. **训练和评估模型**
7. **可视化 ReLU 及其扩展的激活函数**
### 1. 背景介绍
**步骤：**
- 解释 ReLU 在神经网络中的作用和广泛应用。
- 强调 ReLU 的重要性及其在深度学习中的优势。
**解释：**
整流线性单元（ReLU）是一种常用的激活函数，由于其计算简单且能有效缓解梯度消失问题，广泛应用于神经网络尤其是深度学习模型中。ReLU 的主要优势在于其能够使网络快速收敛，并在大多数应用中表现优异。
### 2. 整流线性单元（ReLU）的定义
**步骤：**
- 提供 ReLU 的数学定义。
- 说明 ReLU 如何在神经网络中进行信息处理。
**解释：**
ReLU 的数学定义为：
$$ f(x) = \max(0, x) $$
这意味着如果输入 $ x $ 大于 0，则输出为 $ x $；否则，输出为 0。ReLU 在正半轴上是线性的，在负半轴上是恒为零的。
### 3. ReLU 的优缺点
**步骤：**
- 介绍 ReLU 的主要优点：
  - 计算简单
  - 能有效缓解梯度消失问题
  - 训练速度快
- 讨论 ReLU 的潜在缺点：
  - Dying ReLU 问题
**解释：**
ReLU 的主要优点在于其计算简单，只需进行一次比较操作，这使得它在实际应用中非常高效。此外，ReLU 能有效缓解梯度消失问题，使得神经网络能够在深层结构中保持较好的梯度传递。然而，ReLU 也存在潜在的缺点，如 Dying ReLU 问题，即当大量神经元的输入小于或等于 0 时，这些神经元将不再更新，从而导致网络性能下降。
### 4. ReLU 的扩展
**步骤：**
- 介绍 ReLU 的几种扩展及其定义：
  - Leaky ReLU
  - Parametric ReLU (PReLU)
  - Exponential Linear Unit (ELU)
  - Scaled Exponential Linear Unit (SELU)
**解释：**
- **Leaky ReLU**：允许在负半轴上有一个小的斜率，定义为：
  $$ f(x) = \max(0.01x, x) $$
- **Parametric ReLU (PReLU)**：在负半轴上的斜率是可学习的参数，定义为：
  $$ f(x) = \max(\alpha x, x) $$
  其中，$ \alpha $ 是一个可学习的参数。
- **Exponential Linear Unit (ELU)**：在负半轴上具有指数性质，定义为：
  $$ f(x) = \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha (e^x - 1) & \text{if } x \leq 0 
   \end{cases} $$
- **Scaled Exponential Linear Unit (SELU)**：在 ELU 的基础上进行了缩放，定义为：
  $$ f(x) = \lambda \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha (e^x - 1) & \text{if } x \leq 0 
   \end{cases} $$
  其中，$ \lambda $ 和 $ \alpha $ 是常数，通常设置为特定的值以保证自归一化属性。
### 5. 实现 ReLU 及其扩展
**步骤：**
- 使用 PyTorch 实现 ReLU 及其扩展。
- 演示如何在神经网络中使用这些激活函数。
**代码：**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self, activation='relu'):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'leaky_relu':
            self.activation = nn.LeakyReLU()
        elif activation == 'prelu':
            self.activation = nn.PReLU()
        elif activation == 'elu':
            self.activation = nn.ELU()
        elif activation == 'selu':
            self.activation = nn.SELU()
        self.fc2 = nn.Linear(2, 1)
        self.output = nn.Sigmoid()
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.fc2(x)
        return self.output(x)
# 初始化模型
model = SimpleNN(activation='leaky_relu')
# 定义损失函数和优化器
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
```
### 6. 训练和评估模型
**步骤：**
- 训练模型并记录损失。
- 评估模型在训练数据上的准确性。
**代码：**
```python
# 训练和评估模型的代码已包含在上一步中
```
### 7. 可视化 ReLU 及其扩展的激活函数
**步骤：**
- 可视化不同激活函数的输出。
- 展示不同激活函数在训练过程中的表现差异。
**代码：**
```python
import matplotlib.pyplot as plt
# 可视化训练损失
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss with Leaky ReLU Activation Function')
plt.show()
```
通过以上步骤，我们详细地介绍了整流线性单元（ReLU）及其扩展的概念、类型、选择和实现。每一步都包含了详细的解释和代码示例，帮助理解和掌握 ReLU 及其扩展在神经网络训练中的应用。
# 03_输出单元
"""
Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 03_输出单元
"""
## 03_输出单元
### 任务分解：
1. **背景介绍**
2. **输出单元的定义**
3. **常见的输出单元类型**
4. **选择合适的输出单元**
5. **近似的输出函数**
6. **输出单元在神经网络中的实现**
7. **输出单元的可视化**
### 1. 背景介绍
**步骤：**
- 解释输出单元在神经网络中的作用。
- 强调输出单元对模型预测结果的重要性。
**解释：**
输出单元是神经网络的最后一层，它决定了模型的输出形式和范围。输出单元的选择直接影响模型的预测结果和性能，因此在设计神经网络时，选择合适的输出单元至关重要。
### 2. 输出单元的定义
**步骤：**
- 提供输出单元的数学定义。
- 说明输出单元如何将神经网络的内部计算结果转换为最终的预测结果。
**解释：**
输出单元接收来自前一层的输入（通常是隐藏层的输出），通过激活函数处理后生成最终的输出。不同类型的输出单元适用于不同类型的任务（例如分类、回归等）。
### 3. 常见的输出单元类型
**步骤：**
- 介绍常见的输出单元类型及其适用场景：
  - 恒等函数（Identity Function）
  - Sigmoid 函数
  - Softmax 函数
  - Tanh 函数
**解释：**
- **恒等函数（Identity Function）**：适用于回归问题，输出与输入相同，不进行任何变换。
  
  $$ f(x) = x $$
- **Sigmoid 函数**：适用于二分类问题，将输出值压缩到 [0, 1] 之间。
  
  $$ f(x) = \frac{1}{1 + e^{-x}} $$
- **Softmax 函数**：适用于多分类问题，将输出值转换为概率分布，所有输出值之和为 1。
  
  $$ f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} $$
- **Tanh 函数**：适用于需要输出在 [-1, 1] 之间的情况，常用于隐藏层，但也可用于输出层。
  
  $$ f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
### 4. 选择合适的输出单元
**步骤：**
- 讨论如何根据具体任务选择合适的输出单元。
- 说明不同输出单元对模型性能的影响。
**解释：**
选择合适的输出单元取决于具体的任务类型和数据特性。例如，对于回归问题，恒等函数是合适的选择；对于二分类问题，Sigmoid 函数更为合适；而对于多分类问题，Softmax 函数是最佳选择。选择不当的输出单元可能会导致模型无法正确学习和预测。
### 5. 近似的输出函数
**步骤：**
- 介绍替代 Sigmoid 和 Softmax 的近似函数，降低计算复杂度。
- 说明这些替代函数的优缺点。
**解释：**
- **硬 Sigmoid 函数（Hard Sigmoid Function）**：硬 Sigmoid 函数是一种线性分段近似的 Sigmoid 函数，计算复杂度较低。
  
  $$ f(x) = \max(0, \min(1, \frac{x + 1}{2})) $$
- **Gumbel-Softmax 函数**：用于近似 Softmax 的连续分布，适用于需要对类别进行采样的情况。
**示例：**
硬 Sigmoid 函数的计算复杂度比传统 Sigmoid 函数更低，因为它只涉及简单的线性运算和条件判断。Gumbel-Softmax 函数通过引入噪声项来近似 Softmax 分布，在某些应用中表现良好。
### 6. 输出单元在神经网络中的实现
**步骤：**
- 使用 PyTorch 实现常见的输出单元和近似输出单元。
- 演示如何在神经网络中使用这些输出单元。
**代码：**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
# 定义硬 Sigmoid 函数
class HardSigmoid(nn.Module):
    def forward(self, x):
        return torch.clamp((x + 1) / 2, min=0, max=1)
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self, output_type='identity'):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
        if output_type == 'sigmoid':
            self.output = nn.Sigmoid()
        elif output_type == 'hard_sigmoid':
            self.output = HardSigmoid()
        elif output_type == 'softmax':
            self.output = nn.Softmax(dim=1)
        elif output_type == 'tanh':
            self.output = nn.Tanh()
        else:
            self.output = nn.Identity()
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return self.output(x)
# 初始化模型
model = SimpleNN(output_type='hard_sigmoid')
# 定义损失函数和优化器
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
```
### 7. 输出单元的可视化
**步骤：**
- 可视化不同输出单元的训练损失变化。
- 展示不同输出单元在训练过程中的表现差异。
**代码：**
```python
import matplotlib.pyplot as plt
# 可视化训练损失
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss with Hard Sigmoid Output Unit')
plt.show()
```

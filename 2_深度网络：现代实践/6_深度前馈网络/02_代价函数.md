# 02_代价函数
"""
Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 02_代价函数
"""
## 02_代价函数
### 任务分解：
1. **背景介绍**
2. **代价函数的定义**
3. **常见代价函数**
4. **代价函数的选择**
5. **代价函数在神经网络中的实现**
6. **可视化代价函数**
### 1. 背景介绍
**步骤：**
- 解释代价函数在机器学习和神经网络中的作用。
- 强调代价函数在优化过程中作为目标函数的作用。
**解释：**
代价函数（也称损失函数或误差函数）是评估模型预测与真实值之间差距的函数。在机器学习中，代价函数用于指导模型参数的调整，通过最小化代价函数来优化模型性能。
### 2. 代价函数的定义
**步骤：**
- 提供代价函数的数学定义。
- 说明代价函数如何度量模型预测的好坏。
**解释：**
代价函数 $ J(\theta) $ 是模型参数 $ \theta $ 的函数，表示模型预测与真实值之间的差异。数学上，它通常表示为：
$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}_i, y_i) $$
其中，$ \hat{y}_i $ 是模型的预测值，$ y_i $ 是真实值，$ L $ 是单个样本的损失，$ m $ 是样本数量。
### 3. 常见代价函数
**步骤：**
- 介绍回归问题中的均方误差（MSE）。
- 介绍分类问题中的交叉熵损失（Cross-Entropy Loss）。
- 提及其他常见的代价函数，如绝对误差（MAE）等。
**解释：**
- **均方误差（MSE）**：用于回归问题，计算预测值与真实值之间差值的平方和的平均值。
  
  $$ MSE = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2 $$
- **交叉熵损失（Cross-Entropy Loss）**：用于分类问题，计算预测概率分布与真实标签分布之间的距离。
  
  $$ Cross\_Entropy = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] $$
### 4. 代价函数的选择
**步骤：**
- 讨论如何根据具体问题选择合适的代价函数。
- 说明不同代价函数对模型性能的影响。
**解释：**
选择合适的代价函数取决于具体的任务和数据特性。例如，对于回归问题，均方误差（MSE）通常是首选，而对于分类问题，交叉熵损失（Cross-Entropy Loss）更为合适。选择不当的代价函数可能会导致模型无法有效学习。
### 5. 代价函数在神经网络中的实现
**步骤：**
- 使用 PyTorch 实现常见代价函数。
- 演示如何在训练过程中使用代价函数。
**代码：**
```python
import torch
import torch.nn as nn
import torch.optim as optim
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        x = self.sigmoid(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x
# 初始化模型
model = SimpleNN()
# 定义交叉熵损失函数和优化器
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
```
### 6. 可视化代价函数
**步骤：**
- 可视化训练损失的变化。
- 展示代价函数如何随着训练迭代的增加而变化。
**代码：**
```python
import matplotlib.pyplot as plt
# 可视化训练损失
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
```
### 代价函数误用的影响
### 1. 在分类问题中使用均方误差（MSE）
**问题：**
均方误差（MSE）是用于回归问题的损失函数。当在分类问题中使用 MSE 时，模型可能无法正确学习数据的分类特征。
**影响：**
- **损失函数不适用**：MSE 衡量的是预测值与真实值之间的平方差，而分类问题的目标是最大化预测概率与真实标签的匹配度。MSE 在这种情况下不能有效地捕捉概率分布之间的差异。
- **梯度消失问题**：由于 MSE 的梯度在接近正确分类时会变得非常小，这会导致学习过程变得缓慢甚至停滞。
- **效果不佳**：最终模型的准确性可能会很低，因为它无法正确地最小化分类错误。
**示例：**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        x = self.sigmoid(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x
# 初始化模型
model = SimpleNN()
# 定义均方误差损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
```
在上述示例中，由于使用了不适合分类问题的均方误差（MSE），模型可能无法很好地分类数据。
### 2. 在回归问题中使用交叉熵损失（Cross-Entropy Loss）
**问题：**
交叉熵损失（Cross-Entropy Loss）是用于分类问题的损失函数。当在回归问题中使用交叉熵损失时，模型可能无法正确预测连续值。
**影响：**
- **损失函数不适用**：交叉熵损失衡量的是概率分布之间的差异，而回归问题的目标是预测连续的数值。交叉熵在这种情况下不能有效地衡量预测值与真实值之间的差距。
- **梯度计算错误**：由于交叉熵损失函数设计用于概率值（通常在 [0, 1] 之间），在回归问题中使用时，梯度计算可能会出现问题。
- **效果不佳**：最终模型的预测值可能会不准确，因为它无法正确地最小化回归误差。
**示例：**
```python
# 定义回归问题的简单数据
X = np.array([[1], [2], [3], [4]])
y = np.array([[2], [3], [4], [5]])
# 定义神经网络模型
class RegressionNN(nn.Module):
    def __init__(self):
        super(RegressionNN, self).__init__()
        self.fc1 = nn.Linear(1, 1)
    def forward(self, x):
        return self.fc1(x)
# 初始化模型
model = RegressionNN()
# 定义交叉熵损失函数和优化器（错误使用）
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))  # 错误的损失函数
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float))
    print('Predictions:', predicted.numpy())
```
在上述示例中，由于使用了不适合回归问题的交叉熵损失（Cross-Entropy Loss），模型可能无法正确预测连续值。
### 结论
在不同类型的问题中使用不适当的代价函数会导致模型性能不佳，甚至无法收敛。因此，选择合适的代价函数对于模型的成功训练至关重要。
# 07_其他隐藏单元
"""
Lecture: 2_深度网络：现代实践/6_深度前馈网络
Content: 07_其他隐藏单元
"""
## 07_其他隐藏单元
### 任务分解：
1. **背景介绍**
2. **其他常见隐藏单元类型**
3. **其他隐藏单元的优缺点**
4. **其他隐藏单元的应用场景**
5. **实现其他隐藏单元**
6. **训练和评估模型**
7. **可视化其他隐藏单元的激活函数**
### 1. 背景介绍
**步骤：**
- 解释隐藏单元在神经网络中的作用。
- 强调其他隐藏单元的重要性及其应用场景。
**解释：**
隐藏单元是神经网络的重要组成部分，用于在层与层之间传递信息。除了常见的 ReLU、Sigmoid 和 Tanh，还有其他许多隐藏单元（激活函数）被提出，以解决特定的问题或提高网络的性能。
### 2. 其他常见隐藏单元类型
**步骤：**
- 介绍一些其他常见的隐藏单元类型及其定义：
  - Swish
  - GELU（Gaussian Error Linear Unit）
  - SELU（Scaled Exponential Linear Unit）
  - Mish
**解释：**
- **Swish**：由 Google 提出的激活函数，定义为：
  $$ f(x) = x \cdot \sigma(x) $$
  其中，$ \sigma(x) $ 是 Sigmoid 函数。
- **GELU（Gaussian Error Linear Unit）**：用于 Transformer 模型的激活函数，定义为：
  $$ f(x) = x \cdot \Phi(x) $$
  其中，$ \Phi(x) $ 是标准正态分布的累积分布函数。
- **SELU（Scaled Exponential Linear Unit）**：由自归一化神经网络提出的激活函数，定义为：
  $$ f(x) = \lambda \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha (e^x - 1) & \text{if } x \leq 0 
   \end{cases} $$
  其中，$ \lambda $ 和 $ \alpha $ 是常数。
- **Mish**：由 Self-supervised learning 提出的激活函数，定义为：
  $$ f(x) = x \cdot \tanh(\ln(1 + e^x)) $$
### 3. 其他隐藏单元的优缺点
**步骤：**
- 介绍这些隐藏单元的主要优点。
- 讨论这些隐藏单元的潜在缺点。
**解释：**
- **Swish 优点**：
  - 平滑的非线性
  - 在许多任务中表现优于 ReLU
  
- **Swish 缺点**：
  - 计算量稍大
  
- **GELU 优点**：
  - 更平滑的激活函数
  - 提高了 Transformer 模型的性能
  
- **GELU 缺点**：
  - 计算复杂度较高
  
- **SELU 优点**：
  - 具有自归一化特性，有助于稳定深层神经网络的训练
  
- **SELU 缺点**：
  - 需要特定的参数设置
  
- **Mish 优点**：
  - 保留了 ReLU 的特性，同时具有更好的平滑性和连续性
  
- **Mish 缺点**：
  - 计算复杂度较高
### 4. 其他隐藏单元的应用场景
**步骤：**
- 讨论这些隐藏单元的适用场景。
- 说明何时选择使用这些隐藏单元。
**解释：**
- **Swish** 适用于深层神经网络和需要平滑非线性的任务。
- **GELU** 主要用于 Transformer 和其他自注意力模型中。
- **SELU** 适用于需要稳定和自归一化的深层神经网络。
- **Mish** 适用于各种任务，尤其是需要平滑和连续激活函数的情况。
### 5. 实现其他隐藏单元
**步骤：**
- 使用 PyTorch 实现这些隐藏单元。
- 演示如何在神经网络中使用这些激活函数。
**代码：**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
# 定义 Swish 激活函数
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
# 定义 Mish 激活函数
class Mish(nn.Module):
    def forward(self, x):
        return x * torch.tanh(torch.nn.functional.softplus(x))
# 定义神经网络模型
class SimpleNN(nn.Module):
    def __init__(self, activation='swish'):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        if activation == 'swish':
            self.activation = Swish()
        elif activation == 'gelu':
            self.activation = nn.GELU()
        elif activation == 'selu':
            self.activation = nn.SELU()
        elif activation == 'mish':
            self.activation = Mish()
        self.fc2 = nn.Linear(2, 1)
        self.output = nn.Sigmoid()  # 假设是一个二分类问题
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.fc2(x)
        return self.output(x)
# 初始化模型
model = SimpleNN(activation='mish')
# 定义损失函数和优化器
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
# 准备数据
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
# 训练模型
epochs = 10000
losses = []
for epoch in range(epochs):
    # 前向传播
    outputs = model(torch.tensor(X, dtype=torch.float))
    loss = criterion(outputs, torch.tensor(y, dtype=torch.float))
    losses.append(loss.item())
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 1000 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
# 评估模型
with torch.no_grad():
    predicted = model(torch.tensor(X, dtype=torch.float)).round()
    accuracy = (predicted.numpy() == y).mean()
    print(f'Accuracy: {accuracy * 100:.2f}%')
```
### 6. 训练和评估模型
**步骤：**
- 训练模型并记录损失。
- 评估模型在训练数据上的准确性。
**代码：**
```python
# 训练和评估模型的代码已包含在上一步中
```
### 7. 可视化其他隐藏单元的激活函数
**步骤：**
- 可视化不同激活函数的输出。
- 展示不同激活函数在训练过程中的表现差异。
**代码：**
```python
import matplotlib.pyplot as plt
# 可视化训练损失
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss with Mish Activation Function')
plt.show()
```
### 结论
通过以上步骤，我们详细地介绍了其他常见隐藏单元的概念、定义、优缺点、应用场景及其实现。每一步都包含了详细的解释和代码示例，帮助理解和掌握这些隐藏单元在神经网络训练中的应用。
这些隐藏单元包括：
1. **Swish**：平滑非线性，适用于深层神经网络。
2. **GELU**：主要用于 Transformer 和自注意力模型。
3. **SELU**：适用于需要稳定和自归一化的深层神经网络。
4. **Mish**：平滑和连续的激活函数，适用于各种任务。
每种隐藏单元在不同的任务和网络结构中可能表现出不同的优势，选择合适的激活函数是优化神经网络性能的重要一步。